{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** and then **run all cells**.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "STUDENT_ID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will use your newly discovered classification skills to classify whether a couple is likely to have a successful relationship based on their astrological signs.\n",
    "In this fictional data set, astrological signs do affect compatibility in some ways which you will have to discover. The effects of astrological signs which we have generated for this exercise may differ from real life, in that there are any.\n",
    "\n",
    "Classification is supervised learning, which means that we already know the outcome for some data, and use this data to train a model to classify unknown data.\n",
    "So, in our capacity as spiritual match-makers, we have compiled a list of relationships to train our classifier with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two .csv files with data: couples.csv, and persons.csv.\n",
    "Couples.csv contains the outcome we'd like to predict, but no useful variables to base our prediction on. It only tells us the id of the two partners.\n",
    "It's best if we have both X and Y in a single dataframe before we proceed.\n",
    "Create a dataframe called 'data' with the columns: couple_id, person_a, person_b, outcome, sign_a, sign_b, with the respective signs of persons a and b for each couple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "#print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a look at the possible outcome variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the outcomes\n",
    "couples = pd.read_csv('couples.csv')\n",
    "sns.countplot(data=couples, x='outcome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, one possible outcome for couples is marriage. Although we can train a classifier to classify multiple outcome labels, let's keep it simple for now: We will simply classify whether a couple will be married or not. For this purpose, let's create a one-hot encoding: Create a column for data called 'married' which is equal to 1 if the couple is married and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "#print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to do the same to X: our input features. Since we have two categorical variables (sign_a and sign_b) with 12 possible values each, let's use sklearn's OneHotEncoder class to create vectors instead of manually creating 24 columns. Create a numpy array called 'features' of shape (5000, 24) which contains our one-hot encoded feature vectors.\n",
    "You can follow the first example of the class' documentation:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need: One-hot encoded features and the binary label 'married'.\n",
    "During the lecture you learned about binary logistic regression. Since our output variable is either 0 or 1, this seems like an ideal use case for it!\n",
    "Above, we imported LogisticRegression from sklearn. Have a look at the documentation, and implement it with default parameters for our data:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "Don't worry about test and train sets yet; we'll train our model on all the data and then evaluate it on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "y = data['married']\n",
    "\n",
    "#YOUR CODE HERE\n",
    "\n",
    "classifier.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "78% accuracy! That doesn't sound bad for a first attempt. But remember the zero-rate classifier? Let's have a look at what our baseline should be.\n",
    "Above, we plotted all possible outcome variables. Have a look what's more likely: married (1), or not married (0)? Then create a zero-rate classifier that always returns the most likely result.\n",
    "The estimated outcome value y is often denoted as Å·, which is why we call it y_hat here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_rate(x) -> int:\n",
    "    \"\"\"\n",
    "    Takes the input vector x and then completely disregards it.\n",
    "\n",
    "    Returns:\n",
    "        y(int): Whether or not the couple is married.\n",
    "    \"\"\"\n",
    "\n",
    "    #YOUR CODE HERE\n",
    "\n",
    "y_hat = [zero_rate(x) for x in features]\n",
    "# We imported accuracy_score from sklearn.metrics\n",
    "acc = accuracy_score(y, y_hat)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no, our zero-rate classifier is just as accurate as our logistic regression classifier.\n",
    "It seems our standard logistic regression model could not predict marriage based on astrological signs with any accuracy higher than chance.\n",
    "Does that mean that the signs simply have to effect on courting outcome? That would perhaps be the conclusion if this was astronomy class, but we wouldn't give you such a boring dataset for classification class would we?\n",
    "Assume there is some way in which the signs of the two partners affect their relationship. How come our model did not pick up on those?\n",
    "Can you think of a way in which the features could be re-coded so that logistic regression would predict the outcome with higher accuracy?\n",
    "Hint: A possible solution is shown in the next code cell. Try to think of one yourself first, but if you cannot, figure out why the sample solution works and then explain it here.\n",
    "\n",
    "# Question 1: Why did logistic regression not achieve useful accuracy?\n",
    "\n",
    "YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sign pairs for each possible combination\n",
    "sign_combination = data['sign_a'] + '-' + data['sign_b']\n",
    "# Shape them into an array with n samples and a single feature\n",
    "sign_combination = np.asarray(sign_combination).reshape(-1, 1)\n",
    "# One-hot encode\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(sign_combination)\n",
    "features = enc.transform(sign_combination).toarray()\n",
    "# Train and evaluate our classifier on these new features\n",
    "X = features\n",
    "y = data['married']\n",
    "classifier = LogisticRegression(random_state=0).fit(X, y)\n",
    "classifier.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to have marginally improved the accuracy of our model. Let's see if we can improve it further by building more complex models and taking more information into account."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f9d6cf1e3d8195079a65c851de355134a77367bcd714b1a5d498c42d3c07114"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
